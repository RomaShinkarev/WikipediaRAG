{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b549e444-bb68-4ad9-9f84-97c2a47a1b3e",
   "metadata": {
    "id": "b549e444-bb68-4ad9-9f84-97c2a47a1b3e"
   },
   "source": [
    "# Retrieval‑Augmented Generation (RAG) \n",
    "\n",
    "## О ноутбуке\n",
    "\n",
    "В этом ноутбуке мы реализуем ассистента для ответов на вопросы на основе русской Википедии с помощью RAG, который помимо всего прочего может ходить в интернет за дополнительной информацией. Вся логика системы намеренно написана самостоятельно без специализорованных библиотек. Ноутбук состоит из нескольких логических компонент:\n",
    "\n",
    "1. Векторная база данных с быстрым семантическим поиском\n",
    "2. Разбиение текстов на куски\n",
    "3. Ранжирование документов по релевантности и генерация ответа\n",
    "4. Поиск по интернету\n",
    "5. Ответы, учитывающие контекст\n",
    "\n",
    "## О данных\n",
    "\n",
    "В качестве датасета мы будем использовать [дамп Википедии](https://huggingface.co/datasets/omarkamali/wikipedia-monthly) 1 января 2026 года, который был предварительно выгружен в виде csv файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff2e0ac6-83d3-419b-a01d-110cc40cd8b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:41:36.172392Z",
     "iopub.status.busy": "2026-02-15T19:41:36.172084Z",
     "iopub.status.idle": "2026-02-15T19:41:36.177059Z",
     "shell.execute_reply": "2026-02-15T19:41:36.176325Z",
     "shell.execute_reply.started": "2026-02-15T19:41:36.172364Z"
    },
    "id": "ff2e0ac6-83d3-419b-a01d-110cc40cd8b1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple, Any, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from ddgs import DDGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e00385b0-e865-4e59-ab02-35e079fde06c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123,
     "referenced_widgets": [
      "259e51ad1a374c33a1d81f6fb2a08f85",
      "8e53c6694a1e48b4b4f8cd24f6a8cccd",
      "6188570ca4e54d1fa5ede463aef5c9fc",
      "d97c926f40354718bcb7720554d17e8f",
      "dc1f0b7b63fe4700b351efd78cc9f1cb",
      "4063a9e50b4f4eeba98588b6ad3a96be",
      "a6f5c9d6c530414dac93741c4c19495c",
      "0516d1773c9d4c909d1d2b662dcf8e93",
      "d5eb6e9973074c06b8072aee337d757f",
      "365c1deb446c459aabdcb3d5e606a3f2",
      "0df14472baf74e5a8d35e25037c8c35c"
     ]
    },
    "execution": {
     "iopub.execute_input": "2026-02-15T19:16:48.676524Z",
     "iopub.status.busy": "2026-02-15T19:16:48.675948Z",
     "iopub.status.idle": "2026-02-15T19:16:48.679677Z",
     "shell.execute_reply": "2026-02-15T19:16:48.679029Z",
     "shell.execute_reply.started": "2026-02-15T19:16:48.676498Z"
    },
    "id": "e00385b0-e865-4e59-ab02-35e079fde06c",
    "outputId": "53373ef2-bcfe-4de9-d415-892db1bf9b4a"
   },
   "outputs": [],
   "source": [
    "# Кодом ниже можно скачать полный датасет\n",
    "\n",
    "# data_files = {\n",
    "#     \"train\": \"hf://datasets/omarkamali/wikipedia-monthly/20260101/ru/train/train_part_*.parquet\"\n",
    "# }\n",
    "\n",
    "# dataset = load_dataset(\"parquet\", data_files=data_files, split=\"train\")\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e9ed27-bb90-4528-add4-0f207ecd35b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:16:48.680856Z",
     "iopub.status.busy": "2026-02-15T19:16:48.680617Z",
     "iopub.status.idle": "2026-02-15T19:16:48.692010Z",
     "shell.execute_reply": "2026-02-15T19:16:48.691444Z",
     "shell.execute_reply.started": "2026-02-15T19:16:48.680833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Код ниже очищает текста от \"\\xa0\"\n",
    "\n",
    "# def clean_text(example):\n",
    "#     example[\"text\"] = example[\"text\"].replace(\"\\xa0\", \" \")\n",
    "#     return example\n",
    "\n",
    "# dataset = dataset.map(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22ce6c8-0b5a-4a1a-9db3-87ba0e17f612",
   "metadata": {},
   "source": [
    "Исходный датасет содержит более двух миллионов статей из русскоязычной википедии. Однако для ускорения работы мы будем работать только с 1000 статьями. Они были предварительно выгружены, очищены и сохранены в csv файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f935903c-9324-42b6-9033-43db3619c9f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:16:48.693952Z",
     "iopub.status.busy": "2026-02-15T19:16:48.693488Z",
     "iopub.status.idle": "2026-02-15T19:16:50.053802Z",
     "shell.execute_reply": "2026-02-15T19:16:50.053280Z",
     "shell.execute_reply.started": "2026-02-15T19:16:48.693920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5795e78748ec496bac23dfcd5f967be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/datasets/romashinkarev/wikipedia-ru-10k-20260101/wikipedia_10k_20260101.csv\")[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c903e4e-1556-4c75-9d5e-255269c1273b",
   "metadata": {},
   "source": [
    "Пример текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd2dc62-dc60-4aa2-93e9-6cfc243f10b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:16:50.054867Z",
     "iopub.status.busy": "2026-02-15T19:16:50.054575Z",
     "iopub.status.idle": "2026-02-15T19:16:50.061616Z",
     "shell.execute_reply": "2026-02-15T19:16:50.061015Z",
     "shell.execute_reply.started": "2026-02-15T19:16:50.054843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ки́ров (до 5 декабря 1934 года — Вя́тка, с 1457 по 1780 год такжеВ указанный период (1457—1780) в официально опубликованных нормативно-правовых актах Русского царства и Российской империи город неоднократно именовался Вяткой:\n",
      "\n",
      "А в Вятке прежде благословенного епископа укрепихом и впредь епископу быти.\n",
      "\n",
      "<…> велеть из тех приказов в те городы послать великого государя грамоты <…> на Вятку…\n",
      "\n",
      "<…> да к Вятке 4 пригородка, всего 30 городов.\n",
      "\n",
      "Провинция: Вятская. Вятка…\n",
      "\n",
      "<…> оставить в каждом городе по \n"
     ]
    }
   ],
   "source": [
    "print(dataset[60][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f0fc4e-e777-494c-a651-2d733f84824d",
   "metadata": {},
   "source": [
    "Посмотрим на распределение длин текстов. Нам важно знать с чем мы имеем дело, ведь RAG подразумевает, что мы будем подавать тексты на вход модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af1cd79f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "execution": {
     "iopub.execute_input": "2026-02-15T19:16:50.062652Z",
     "iopub.status.busy": "2026-02-15T19:16:50.062391Z",
     "iopub.status.idle": "2026-02-15T19:16:50.756194Z",
     "shell.execute_reply": "2026-02-15T19:16:50.755528Z",
     "shell.execute_reply.started": "2026-02-15T19:16:50.062618Z"
    },
    "id": "af1cd79f",
    "outputId": "69408861-1366-422b-d2d0-6a49cf7702a6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens = np.array([len(dataset[i]['text'].split()) for i in range(len(dataset))])\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(lens, bins=50)\n",
    "plt.title('Распределение длин текстов')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(lens[lens < 1000], bins=50)\n",
    "plt.title('Распределение длин текстов короче 1000 слов')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be8bd5",
   "metadata": {
    "id": "45be8bd5"
   },
   "source": [
    "Видим, что тексты часто довольно длинные. Поэтому смухлевать не получится и перед векторизацией нужно будет разбивать каждый текст на небольшие куски, чтобы не перегружать контекст модели и чтобы не потерять важную информацию при векторизации текстов на этапе складывания их в векторную базу данных.\n",
    "\n",
    "Приступим к реализации RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeabe13",
   "metadata": {
    "id": "bdeabe13"
   },
   "source": [
    "## Загрузка модели\n",
    "\n",
    "В качестве основы для ассистента возьмем модель [`Qwen/Qwen2.5-3B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) из huggingface. Важно, что это `Instruct` модель, то есть она предлазначена для общения в формате диалога.\n",
    "\n",
    "Проверим как модель отвечает на кулинарные вопросы без RAG. Это необходимо сделать, чтобы понять, нужен ли вообще RAG для данной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba3c508d-7db6-424a-94c5-2bfd85ee497a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390,
     "referenced_widgets": [
      "59444daf4a5749a896056a79696a4325",
      "0dd1375fc9dc424eaec5e43cbe0e6fc2",
      "d4f47baeb26d485892efa6937358776f",
      "65ff741c4857483b9d54586225923401",
      "1bb7be1a91f845da9200ff83f312c20b",
      "48d9f13da4a847b59d53df4d0d7d396f",
      "ba5b42227f684455a3bb22076ff8c15d",
      "9a050e41a0af49c2858cf7152fc49ab2",
      "f25b7d727dbb4d9ba686800fc5cac912",
      "0e2d9d6f649c4364b4611bd6e1deac4d",
      "e15932c25b564e15808ffab963aba2f6",
      "238310de69454feeb20a77512bfd4908",
      "2b10feb58ab54aa0a6d4741d5f84132c",
      "6e1dc47f21144b2092eddd755512d91c",
      "825158027f5b4fcc9b16b2114020e535",
      "a7c59145cc844ad4beab82c4e5496c0d",
      "7f2a86132d454f73b09b5c796eae1779",
      "ded281de88f243a98df54a192151d009",
      "dedf8a0983c24990859e6986d52a631e",
      "381ff2036e01436e9ea5aaa1bd8f11e7",
      "258e72c3fb704d6e81cf62798fccde07",
      "ddfdd8fa7e4146b687d834782f44e4a4",
      "19227087e7ea4a5aa32a0ebc08b8762b",
      "7113800f454a458e8f0e69dc6cf42c84",
      "77bdd445ae9e44409dc3a5643f9a39f7",
      "912f8e2c22df496087836af00d67afac",
      "13e1742f351d4504a9a70a33221d0ac5",
      "7dff8e2c19dd4bb399bc6b7dca129f5d",
      "62d2a76765b144d9a2a487523689d32f",
      "db9be252dbbe473290110b4067c2ffa4",
      "4b7886d553d64ba991d35958724a2671",
      "f73592d074ee485f8b09fbee715721d2",
      "1e7483bb50124c97817f925ddf367e61",
      "958f1e78716341e0a65f8ef8c42915e9",
      "4c84106a623e4bc58f1d034dd797aee1",
      "36f4eabddec44ef091749f54af288e6d",
      "83cd3d671607404c9583867a0ab75465",
      "f1b8cfdf71904c86bb24bd554907db87",
      "4d02485073ad4ab5b999dbb2da925c5d",
      "93a5b516d3014acd9afa7ee35c487898",
      "84d1649a13f94949b79867364c7d0f49",
      "a495139cc8d14c5bb1aaed6ffa0f5f80",
      "a29b36cd54cd4ce095b0b82d9e1367c3",
      "2ced124a21ac4efe919b98a5f3c4b19a",
      "8b47cf07c4504f8a88d33ef2b42a954e",
      "a30ebb4e2ad24b838d03f7dbd08fe389",
      "0af1c2aaebde47079ad42dca48f1a1c2",
      "bb2f522663184f0490aeecc252276ca2",
      "01e37c7dff1c49648ddade68791d00a6",
      "a9ddae8076be4b1d86ba11ba5d2d93ff",
      "88261afaa01f4619ab1b3dffc56c170b",
      "e3220129c203490098b75ee863d4ab0d",
      "775a274cf091417985d40a319b7dc9e5",
      "e1c0c7718c5744be984f134d68ab8414",
      "496cdba74a6f47f0b963a67ec134f130",
      "a9425746de9d4f41a32a2b5c4cbbf270",
      "89826d7f40f34315b7740b928e6fff9e",
      "7bf8a86bbf754d7588db0d60e9720ee1",
      "9d9877c5ad704d8bab43db2a5220b3f5",
      "39ced1b7c0e9407ba51ef50e32b91586",
      "088128fa130547afb14c63eaa28c5112",
      "5581fac20069446d9244fbfd33f3f68e",
      "946dc9ee7f584eb78d8f1a4c727b6d06",
      "1c4a7e006fa94647bef7299fed3d44f9",
      "ba4756e4ee914ea6b23042126989ed35",
      "35034542daa9428eb94e39de4dc4e1f6",
      "ff6abc0a063340739ee58271d2b0dff3",
      "f18cc216c12b459b8c01a701ec053515",
      "05dbf04883bd4a5f8022c9b15ea66cf3",
      "3edaa3bd75004c7487adb256eb0e4431",
      "27cf7eba08634cc7845a303928d03ec6",
      "06b7170466154f168cd78296c8701e21",
      "8087169ee4194b2ca20105399cc15c6a",
      "ca66034d863b47b0ba94ad146f784451",
      "549dbcfa3d0748d2bf59961c05e12ab2",
      "559870768c6147c3b3f45b4e7db86b0a",
      "e50ea8893be94e01a1fb918257c1bec7"
     ]
    },
    "execution": {
     "iopub.execute_input": "2026-02-15T19:16:50.757234Z",
     "iopub.status.busy": "2026-02-15T19:16:50.757037Z",
     "iopub.status.idle": "2026-02-15T19:17:43.798500Z",
     "shell.execute_reply": "2026-02-15T19:17:43.797937Z",
     "shell.execute_reply.started": "2026-02-15T19:16:50.757214Z"
    },
    "id": "ba3c508d-7db6-424a-94c5-2bfd85ee497a",
    "outputId": "6933122b-be75-48f0-d21a-770faf8563e5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20aa5806dcfd43d1a2376f3b503bd844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13400756b0640368be47831ca528c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce4d3cf4a4a4b938c920c0864d2d60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80a2ca8fcd74178ad84c29f3ef88f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25d75f563ae4b6f9b14689f6e6c80a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-15 19:16:54.612411: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771183014.784721      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771183014.832456      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771183015.247818      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771183015.247848      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771183015.247851      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771183015.247853      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08070fa57bd941f198e483688717e60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3935673f3db74e5c91d7c0d523982e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8495778cc524cc3a84ceb566b60a766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99b0efcf16848a883236eda4552e820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f749d293cd40ac8103532a4065b138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d3d609867c45bca7acb76ae3583479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "generation_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "708edf53-72c6-4170-9727-479bd209611d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:17:43.800205Z",
     "iopub.status.busy": "2026-02-15T19:17:43.799484Z",
     "iopub.status.idle": "2026-02-15T19:17:43.805584Z",
     "shell.execute_reply": "2026-02-15T19:17:43.804935Z",
     "shell.execute_reply.started": "2026-02-15T19:17:43.800179Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_answer(question, generation_model, generation_tokenizer, max_new_tokens=512, temperature=0.7, top_p=0.9):   \n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    \n",
    "    text = generation_tokenizer.apply_chat_template(messages, \n",
    "                                                    tokenize=False, \n",
    "                                                    add_generation_prompt=True)\n",
    "    inputs = generation_tokenizer(text, return_tensors=\"pt\").to(generation_model.device)\n",
    "\n",
    "    prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = generation_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=generation_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    new_tokens = out[0, prompt_len:]\n",
    "    return generation_tokenizer.decode(new_tokens, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f0cb878-bca5-420c-846f-0a1f06991f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:17:43.806736Z",
     "iopub.status.busy": "2026-02-15T19:17:43.806470Z",
     "iopub.status.idle": "2026-02-15T19:17:53.319345Z",
     "shell.execute_reply": "2026-02-15T19:17:53.318646Z",
     "shell.execute_reply.started": "2026-02-15T19:17:43.806703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Как раньше назывался город Киров?\n",
      "Ответ: Город Киров не менял своего названия в течение истории. Он был таковым с момента его основания в 1930 году. Так что ответ на ваш вопрос - название у него не менялось.\n"
     ]
    }
   ],
   "source": [
    "query = \"Как раньше назывался город Киров?\" \n",
    "print(f\"Запрос: {query}\")\n",
    "answer = generate_answer(query, generation_model, generation_tokenizer)\n",
    "print(f\"Ответ: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24d21bf2-4ef0-4160-8eaa-e915e8e2ad23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:17:53.323507Z",
     "iopub.status.busy": "2026-02-15T19:17:53.323208Z",
     "iopub.status.idle": "2026-02-15T19:18:00.879639Z",
     "shell.execute_reply": "2026-02-15T19:18:00.878964Z",
     "shell.execute_reply.started": "2026-02-15T19:17:53.323483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Где располагался административный центр войска Донского?\n",
      "Ответ: Административный центр войска Донского располагался в городе Курган-Уральский (ранее известный как Новочеркасск) в至今为止的俄罗斯。不过，请注意，这一信息可能需要根据最新的历史和地理资料进行验证，因为行政区划可能会有所变动。\n",
      "\n",
      "在历史上，16-17世纪，这个区域属于喀山楚克（Kazan Chuch），后来在1654年，喀山楚克被沙皇俄国吞并，成为俄罗斯的一部分，并在此基础上形成了现在的顿河畔罗斯托夫州，其首府也相应地从喀山楚克迁移到了罗斯托夫。\n",
      "\n",
      "如果您需要确切的信息，建议查阅最新的历史或地方志资料。\n"
     ]
    }
   ],
   "source": [
    "query = \"Где располагался административный центр войска Донского?\" \n",
    "print(f\"Запрос: {query}\")\n",
    "answer = generate_answer(query, generation_model, generation_tokenizer)\n",
    "print(f\"Ответ: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e196906-8b15-4e71-a918-cc6b6147cd52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:00.880898Z",
     "iopub.status.busy": "2026-02-15T19:18:00.880578Z",
     "iopub.status.idle": "2026-02-15T19:18:13.280170Z",
     "shell.execute_reply": "2026-02-15T19:18:13.279317Z",
     "shell.execute_reply.started": "2026-02-15T19:18:00.880848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Среднегодовая температура воздуха в России\n",
      "Ответ: Среднегодовая температура воздуха в России сильно зависит от региона. В целом, среднегодовая температура по всей стране составляет около -2°C, но это очень общее значение и не учитывает местные особенности.\n",
      "\n",
      "В северных регионах, таких как Сибирь и Забайкалье, среднегодовая температура ниже нуля. Например, в Якутии она может быть около -20°C.\n",
      "\n",
      "В центральной России среднегодовая температура обычно находится выше нуля, но колеблется в зависимости от места. Например, в Москве среднегодовая температура составляет около +3°C.\n",
      "\n",
      "В южных регионах, таких как Средняя Азия и Кавказ, среднегодовая температура выше нуля. Например, в Казани (Удмуртской Республики) она составляет около +5°C.\n",
      "\n",
      "Это общие данные, и точную информацию о среднегодовой температуре в конкретном регионе лучше всего получить из официальных метеорологических данных для того года, который интересует.\n"
     ]
    }
   ],
   "source": [
    "query = \"Среднегодовая температура воздуха в России\" \n",
    "print(f\"Запрос: {query}\")\n",
    "answer = generate_answer(query, generation_model, generation_tokenizer)\n",
    "print(f\"Ответ: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b65c6-5c14-450b-9e45-5af1a99ff7ba",
   "metadata": {},
   "source": [
    "Как мы видим модель либо отвечает неправильно и выдумывает, либо не может точно ответить на вопрос."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466d510",
   "metadata": {
    "id": "5466d510"
   },
   "source": [
    "## Locality Sensitive Hashing (LSH)\n",
    "\n",
    "В RAG для каждого запроса необходимо искать релевантные документы по векторной базе данных. Так как документов очень много, подсчет релевантности для каждого из них будет занимать очень много времени. Поэтому нам важно, чтобы поиск был быстрым. Для достижения этого мы даже готовы немного пожертвовать точностью.\n",
    "\n",
    "В этой секции реализован [Locality Sensitive Hashing](https://ru.wikipedia.org/wiki/Locality-sensitive_hashing).\n",
    "\n",
    "Идея LSH довольна проста. Разделим все векторы из базы данных на группы, состоящие из похожих векторов. В начале поиска определим группу, которая соответствует вектору запроса и будем считать релевантность только для объектов из этой группы. Для разбиения на группы мы введем несколько хеш-функций. Каждая их них будет строиться по следующему алгоритму:\n",
    "1. Сгенерируем $k$ случайных векторов (векторы нормали гиперплоскости)\n",
    "2. Для отдельной точки (вектора) в базе данных и вектора нормали запишем 1, если точка лежит над соответствующей гиперплоскостью. В обратном случае запишем 0.\n",
    "3. Повторим процедуру для всех точек и векторов нормали. Так мы для каждой точки получим список из $k$ 0 и 1, который и будет хешем.\n",
    "4. Запишем в хеш-таблицу найденные точки для каждого хеша.\n",
    "5. Повторим шаги 1-4 $L$ раз и получим $L$ разных хеш-таблиц.\n",
    "\n",
    "Теперь для каждого нового вектора $q$ мы можем очень быстро найти набор векторов похожих на него. Для этого посчитаем хеши запроса в каждой хеш-таблице и возьмем все точки, у которых хеш совпал c $q$ хотя бы в одной из них. Затем переранжируем найденные точки по релевантности (расстоянию до вектора запроса) и оставим заданное число самых близких.\n",
    "\n",
    "Сложность формирования хеш-таблиц можно оценить как $O(nLkd)$, где $n$ – число векторов в базе данных, а $d$ – размерность вектора. В то же время сложность поиска LSH – $O(Lkd + (nLp) \\cdot d)$, где $p$ – вероятность того, что хеш двух случайных точек совпадет ($p$ стремится к 0). Сложность полного перебора – $O(nd)$, поэтому при поиске LSH работает быстрее почти в $\\frac{n}{Lk}$ раз!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00913d4-aaf6-42e7-84d1-373612e7ddb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:13.281242Z",
     "iopub.status.busy": "2026-02-15T19:18:13.281044Z",
     "iopub.status.idle": "2026-02-15T19:18:13.294058Z",
     "shell.execute_reply": "2026-02-15T19:18:13.293146Z",
     "shell.execute_reply.started": "2026-02-15T19:18:13.281222Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSHVectorDB:\n",
    "    def __init__(self, L: int = 15, k: int = 12, d: int = 1024, seed: int = 42):\n",
    "        self.L = L\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        self.seed = seed\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.hash_tables = []\n",
    "        self.normal_vectors = []\n",
    "        \n",
    "        for _ in range(L):\n",
    "            normals = np.random.randn(k, d)\n",
    "            normals = normals / np.linalg.norm(normals, axis=1, keepdims=True)\n",
    "            self.normal_vectors.append(normals)\n",
    "            self.hash_tables.append(defaultdict(list))\n",
    "        \n",
    "        self.vectors = []\n",
    "        self.metadata = []\n",
    "        \n",
    "    def _compute_hash(self, vector: np.ndarray, table_idx: int) -> str:\n",
    "        normals = self.normal_vectors[table_idx]\n",
    "        hash_bits = (np.dot(normals, vector) >= 0).astype(int)\n",
    "        return ''.join(hash_bits.astype(str))\n",
    "    \n",
    "    def add_vector(self, vector: np.ndarray, metadata: Optional[dict] = None):\n",
    "        vector = np.array(vector, dtype=np.float32)\n",
    "        if vector.shape[0] != self.d:\n",
    "            raise ValueError(f\"Вектор должен быть размерности {self.d}, получен {vector.shape[0]}\")\n",
    "        \n",
    "        vector = vector / np.linalg.norm(vector)\n",
    "        \n",
    "        vector_idx = len(self.vectors)\n",
    "        self.vectors.append(vector)\n",
    "        self.metadata.append(metadata)\n",
    "        \n",
    "        for table_idx in range(self.L):\n",
    "            hash_key = self._compute_hash(vector, table_idx)\n",
    "            self.hash_tables[table_idx][hash_key].append(vector_idx)\n",
    "    \n",
    "    def search(self, query_vector: np.ndarray, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "        query_vector = np.array(query_vector, dtype=np.float32)\n",
    "        if query_vector.shape[0] != self.d:\n",
    "            raise ValueError(f\"Вектор запроса должен быть размерности {self.d}\")\n",
    "        \n",
    "        query_vector = query_vector / np.linalg.norm(query_vector)\n",
    "        \n",
    "        candidate_indices = set()\n",
    "        \n",
    "        for table_idx in range(self.L):\n",
    "            hash_key = self._compute_hash(query_vector, table_idx)\n",
    "            if hash_key in self.hash_tables[table_idx]:\n",
    "                candidate_indices.update(self.hash_tables[table_idx][hash_key])\n",
    "        \n",
    "        if len(candidate_indices) == 0:\n",
    "            return []\n",
    "        \n",
    "        candidates = list(candidate_indices)\n",
    "        candidate_vectors = np.array([self.vectors[i] for i in candidates])\n",
    "        \n",
    "        similarities = np.dot(candidate_vectors, query_vector)\n",
    "        \n",
    "        sorted_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = [(candidates[i], float(similarities[i])) for i in sorted_indices]\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74abb044-4ada-4090-bdb6-e07b91cc7620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:13.295399Z",
     "iopub.status.busy": "2026-02-15T19:18:13.295098Z",
     "iopub.status.idle": "2026-02-15T19:18:15.865476Z",
     "shell.execute_reply": "2026-02-15T19:18:15.864707Z",
     "shell.execute_reply.started": "2026-02-15T19:18:13.295357Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_vector_db(save_dir: str = \"vector_db\"):    \n",
    "    with open(f\"{save_dir}/lsh_db.pkl\", \"rb\") as f:\n",
    "        db_data = pickle.load(f)\n",
    "    \n",
    "    vector_db = LSHVectorDB(\n",
    "        L=db_data['L'],\n",
    "        k=db_data['k'],\n",
    "        d=db_data['d'],\n",
    "        seed=db_data['seed']\n",
    "    )\n",
    "    vector_db.hash_tables = db_data['hash_tables']\n",
    "    vector_db.normal_vectors = db_data['normal_vectors']\n",
    "    vector_db.vectors = db_data['vectors']\n",
    "    \n",
    "    with open(f\"{save_dir}/metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    vector_db.metadata = metadata\n",
    "    \n",
    "    return vector_db, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cacde53",
   "metadata": {
    "id": "1cacde53"
   },
   "source": [
    "## Разбиение текста на куски\n",
    "\n",
    "Так как некоторые тексты довольно длинные, если мы будем кодировать их целиком одним вектором, то часть информации потеряется, так как вектор имеет ограниченный размер. Чтобы не терять информацию, мы поделим текст на куски и будем кодировать каждый кусок отдельно.\n",
    "\n",
    "Реализуем [рекурсивное разбиение](https://dev.to/eteimz/understanding-langchains-recursivecharactertextsplitter-2846) текста с уровнями `[\"\\n\\n\", \"\\n\", \" \", \"\"]` и пересечением между кусками для уровней `[\" \", \"\"]`. То есть если целый абзац помещается в один кусок, то пересекать его ни с чем не надо (в нем содержится законченная мысль), а если абзац не влезает и его приходится делить по словам (или символам), то нужно добавить пересечение со словами из соседних кусков того же абзаца."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79e5619a-ab25-40bd-b6bd-0bb80eca88fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:15.866775Z",
     "iopub.status.busy": "2026-02-15T19:18:15.866446Z",
     "iopub.status.idle": "2026-02-15T19:18:15.879077Z",
     "shell.execute_reply": "2026-02-15T19:18:15.878428Z",
     "shell.execute_reply.started": "2026-02-15T19:18:15.866741Z"
    }
   },
   "outputs": [],
   "source": [
    "class RecursiveTextSplitter:\n",
    "    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 256):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        \n",
    "    def _split_by_separator(self, text: str, separator: str) -> List[str]:\n",
    "        if separator == \"\":\n",
    "            return list(text)\n",
    "        return text.split(separator)\n",
    "    \n",
    "    def _count_words(self, text: str) -> int:\n",
    "        return len(text.split())\n",
    "    \n",
    "    def _split_text_recursive(self, text: str, separators: List[str], \n",
    "                              current_chunks: List[str] = None) -> List[str]:\n",
    "        if current_chunks is None:\n",
    "            current_chunks = []\n",
    "        \n",
    "        if not separators:\n",
    "            if text.strip():\n",
    "                current_chunks.append(text.strip())\n",
    "            return current_chunks\n",
    "        \n",
    "        separator = separators[0]\n",
    "        remaining_separators = separators[1:]\n",
    "        splits = self._split_by_separator(text, separator)\n",
    "        \n",
    "        good_splits = []\n",
    "        current_split = \"\"\n",
    "        \n",
    "        for split in splits:\n",
    "            if separator != \"\":\n",
    "                test_text = current_split + split + (separator if split != splits[-1] else \"\")\n",
    "            else:\n",
    "                test_text = current_split + split\n",
    "            \n",
    "            word_count = self._count_words(test_text) if separator != \"\" else len(test_text)\n",
    "            \n",
    "            if word_count <= self.chunk_size:\n",
    "                current_split = test_text\n",
    "            else:\n",
    "                if current_split.strip():\n",
    "                    good_splits.append(current_split.strip())\n",
    "                \n",
    "                if self._count_words(split) > self.chunk_size if separator != \"\" else len(split) > self.chunk_size:\n",
    "                    sub_chunks = self._split_text_recursive(split, remaining_separators, [])\n",
    "                    good_splits.extend(sub_chunks)\n",
    "                    current_split = \"\"\n",
    "                else:\n",
    "                    current_split = split + (separator if separator != \"\" else \"\")\n",
    "        \n",
    "        if current_split.strip():\n",
    "            good_splits.append(current_split.strip())\n",
    "        \n",
    "        needs_overlap = separator in [\" \", \"\"]\n",
    "        \n",
    "        if needs_overlap and len(good_splits) > 1:\n",
    "            final_chunks = []\n",
    "            for i, chunk in enumerate(good_splits):\n",
    "                if i == 0:\n",
    "                    final_chunks.append(chunk)\n",
    "                else:\n",
    "                    prev_chunk = good_splits[i-1]\n",
    "                    prev_words = prev_chunk.split() if separator == \" \" else list(prev_chunk)\n",
    "                    overlap_size = min(self.chunk_overlap, len(prev_words))\n",
    "                    \n",
    "                    if separator == \" \":\n",
    "                        overlap_text = \" \".join(prev_words[-overlap_size:])\n",
    "                        new_chunk = overlap_text + \" \" + chunk if overlap_text else chunk\n",
    "                    else:\n",
    "                        overlap_text = \"\".join(prev_words[-overlap_size:])\n",
    "                        new_chunk = overlap_text + chunk if overlap_text else chunk\n",
    "                    \n",
    "                    final_chunks.append(new_chunk)\n",
    "            \n",
    "            return final_chunks\n",
    "        \n",
    "        return good_splits\n",
    "    \n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        \n",
    "        chunks = self._split_text_recursive(text, self.separators.copy())\n",
    "        final_chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "        \n",
    "        return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "775ce380-a8ba-4891-865b-56560580723e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:15.880114Z",
     "iopub.status.busy": "2026-02-15T19:18:15.879855Z",
     "iopub.status.idle": "2026-02-15T19:18:15.892583Z",
     "shell.execute_reply": "2026-02-15T19:18:15.892078Z",
     "shell.execute_reply.started": "2026-02-15T19:18:15.880092Z"
    }
   },
   "outputs": [],
   "source": [
    "test_text = dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8f50e24-6796-4ac6-a22c-79aa67679e63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:15.894415Z",
     "iopub.status.busy": "2026-02-15T19:18:15.894133Z",
     "iopub.status.idle": "2026-02-15T19:18:15.907248Z",
     "shell.execute_reply": "2026-02-15T19:18:15.906525Z",
     "shell.execute_reply.started": "2026-02-15T19:18:15.894394Z"
    }
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveTextSplitter(chunk_size=512, chunk_overlap=256)\n",
    "chunks = splitter.split_text(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c37a67d-9488-459a-936f-a8fef4f5c625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:15.908469Z",
     "iopub.status.busy": "2026-02-15T19:18:15.908172Z",
     "iopub.status.idle": "2026-02-15T19:18:15.919601Z",
     "shell.execute_reply": "2026-02-15T19:18:15.918906Z",
     "shell.execute_reply.started": "2026-02-15T19:18:15.908436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кусок 1 (495 слов):\n",
      "Литва́ ( ), официальное название — Лито́вская Респ...\n",
      "\n",
      "Кусок 2 (472 слов):\n",
      "Климат — переходный от морского к континентальному...\n",
      "\n",
      "Кусок 3 (486 слов):\n",
      "В 1385 году великий князь литовский Ягайло заключи...\n",
      "\n",
      "Кусок 4 (452 слов):\n",
      "Впоследствии литовцы вновь попытались восстановить...\n",
      "\n",
      "Кусок 5 (473 слов):\n",
      "После поражения Красной армии под Варшавой и совет...\n",
      "\n",
      "Кусок 6 (442 слов):\n",
      "14—15 июля 1940 года, после ввода дополнительного ...\n",
      "\n",
      "Кусок 7 (481 слов):\n",
      "18 апреля 1990 года СССР ввёл экономическую блокад...\n",
      "\n",
      "Кусок 8 (488 слов):\n",
      "Литва является светским государством. Состав насел...\n",
      "\n",
      "Кусок 9 (494 слов):\n",
      "Парламент республики — однопалатный Сейм Литовской...\n",
      "\n",
      "Кусок 10 (471 слов):\n",
      "В Литве открытая и смешанная экономика, которую Вс...\n",
      "\n",
      "Кусок 11 (383 слов):\n",
      "В Литве плоская шкала налогообложения. По данным Е...\n",
      "\n",
      "Кусок 12 (503 слов):\n",
      "Основание Вильнюсского университета в 1579 году яв...\n",
      "\n",
      "Кусок 13 (497 слов):\n",
      "В 2008 году была начата программа развития инновац...\n",
      "\n",
      "Кусок 14 (467 слов):\n",
      "Статус \n",
      "В отличие от других традиционных вооружённ...\n",
      "\n",
      "Кусок 15 (222 слов):\n",
      "Союз образован в 1919 году в качестве ополчения. Л...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks, 1):\n",
    "    word_count = len(chunk.split())\n",
    "    print(f\"Кусок {i} ({word_count} слов):\")\n",
    "    print(f\"{chunk[:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5441c",
   "metadata": {
    "id": "5ca5441c"
   },
   "source": [
    "## RAG\n",
    "\n",
    "Наконец мы можем начать собирать систему RAG. Для начала соберем векторую базу данных из кусков рецептов. Для получения эмбеддингов текстов возьмем модель [`intfloat/multilingual-e5-large`](https://huggingface.co/intfloat/multilingual-e5-large), она создана специально для этой задачи и хорошо работает с русским языком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d854e894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:15.921016Z",
     "iopub.status.busy": "2026-02-15T19:18:15.920525Z",
     "iopub.status.idle": "2026-02-15T19:18:37.027959Z",
     "shell.execute_reply": "2026-02-15T19:18:37.026924Z",
     "shell.execute_reply.started": "2026-02-15T19:18:15.920989Z"
    },
    "id": "d854e894"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d047a33e4d47758385e4332bf32fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73a67c1afb14d3fbbc2bb4060671fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1aaedcd364410792ccbf540410955f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18488fd39e38488bb2e05b36c9a07ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcf2f00ea6b41969e566e1f018c68c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07426ab0e1484223875a1e197bdcf270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb59cfd51af4e35b7cdb43de3184efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(8194, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_name = \"BAAI/bge-m3\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = embedding_model.to(device)\n",
    "embedding_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9ab4bef-29f1-4db0-8046-30845093b582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T20:23:21.014327Z",
     "iopub.status.busy": "2026-02-15T20:23:21.013627Z",
     "iopub.status.idle": "2026-02-15T20:23:24.883237Z",
     "shell.execute_reply": "2026-02-15T20:23:24.882608Z",
     "shell.execute_reply.started": "2026-02-15T20:23:21.014295Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Разбиение статей: 100%|██████████| 1000/1000 [00:03<00:00, 259.04it/s]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveTextSplitter(chunk_size=128, chunk_overlap=64)\n",
    "\n",
    "all_chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for i in tqdm(range(len(dataset)), desc=\"Разбиение статей\"):\n",
    "    article = dataset[i]\n",
    "    text = article['text']\n",
    "    title = article['title']\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(chunk)\n",
    "        chunk_metadata.append({\n",
    "            'article_idx': i,\n",
    "            'article_name': title,\n",
    "            'chunk_text': chunk\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d426d14-f84e-42b9-b9c2-8de9462ff028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:44.562676Z",
     "iopub.status.busy": "2026-02-15T19:18:44.562406Z",
     "iopub.status.idle": "2026-02-15T19:18:44.569180Z",
     "shell.execute_reply": "2026-02-15T19:18:44.568590Z",
     "shell.execute_reply.started": "2026-02-15T19:18:44.562653Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(\n",
    "    texts: List[str],\n",
    "    embedding_model=None,\n",
    "    embedding_tokenizer=None,\n",
    "    device=None,\n",
    "    batch_size: int = 32,\n",
    "    max_length: int = 512\n",
    ") -> np.ndarray:\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Векторизация\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        inputs = embedding_tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**inputs)\n",
    "            token_emb = outputs.last_hidden_state\n",
    "            mask = inputs[\"attention_mask\"].unsqueeze(-1).float()\n",
    "\n",
    "            summed = (token_emb * mask).sum(dim=1)\n",
    "            counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "            emb = summed / counts\n",
    "\n",
    "            emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "\n",
    "        all_embeddings.append(emb.detach().cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f02f1bda-0db4-43dd-a434-63d24c281eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:44.570199Z",
     "iopub.status.busy": "2026-02-15T19:18:44.569973Z",
     "iopub.status.idle": "2026-02-15T19:36:31.863578Z",
     "shell.execute_reply": "2026-02-15T19:36:31.862706Z",
     "shell.execute_reply.started": "2026-02-15T19:18:44.570177Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1027/1027 [17:39<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_embeddings(all_chunks, embedding_model, embedding_tokenizer, device, 32)\n",
    "embedding_dim = embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccd1b24b-926c-422f-9830-e9e0cd90b187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:36:31.865221Z",
     "iopub.status.busy": "2026-02-15T19:36:31.864668Z",
     "iopub.status.idle": "2026-02-15T19:36:45.745329Z",
     "shell.execute_reply": "2026-02-15T19:36:45.744512Z",
     "shell.execute_reply.started": "2026-02-15T19:36:31.865194Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32833/32833 [00:13<00:00, 2368.41it/s]\n"
     ]
    }
   ],
   "source": [
    "L = 20\n",
    "k = 15\n",
    "\n",
    "vector_db = LSHVectorDB(L=L, k=k, d=embedding_dim)\n",
    "\n",
    "for embedding, metadata in tqdm(zip(embeddings, chunk_metadata), total=len(embeddings)):\n",
    "    vector_db.add_vector(embedding, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acc8464b-ed74-4f94-9979-daf099777c31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:36:45.746968Z",
     "iopub.status.busy": "2026-02-15T19:36:45.746543Z",
     "iopub.status.idle": "2026-02-15T19:36:46.113726Z",
     "shell.execute_reply": "2026-02-15T19:36:46.113145Z",
     "shell.execute_reply.started": "2026-02-15T19:36:45.746922Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dir = \"vector_db\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{save_dir}/lsh_db.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        'hash_tables': vector_db.hash_tables,\n",
    "        'normal_vectors': vector_db.normal_vectors,\n",
    "        'vectors': vector_db.vectors,\n",
    "        'L': vector_db.L,\n",
    "        'k': vector_db.k,\n",
    "        'd': vector_db.d,\n",
    "        'seed': vector_db.seed\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40be8005-f987-4eb7-8f6f-dc299b264e1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:36:46.114834Z",
     "iopub.status.busy": "2026-02-15T19:36:46.114563Z",
     "iopub.status.idle": "2026-02-15T19:36:46.547718Z",
     "shell.execute_reply": "2026-02-15T19:36:46.546926Z",
     "shell.execute_reply.started": "2026-02-15T19:36:46.114797Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"{save_dir}/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunk_metadata, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bba8d9b-4751-4e1d-9839-e92e22581d9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:36:46.549038Z",
     "iopub.status.busy": "2026-02-15T19:36:46.548743Z",
     "iopub.status.idle": "2026-02-15T19:36:46.598311Z",
     "shell.execute_reply": "2026-02-15T19:36:46.597045Z",
     "shell.execute_reply.started": "2026-02-15T19:36:46.549004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Как раньше назывался город Киров?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 34.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найденные результаты:\n",
      "\n",
      "Результат №1. Сходство: 0.783\n",
      "Заголовок: Киров\n",
      "Текст: Население города —  чел. ( год), а городского окру...\n",
      "\n",
      "Результат №2. Сходство: 0.775\n",
      "Заголовок: Киров (значения)\n",
      "Текст: Россия\n",
      " Киров — город, административный центр Киро...\n",
      "\n",
      "Результат №3. Сходство: 0.773\n",
      "Заголовок: Киров\n",
      "Текст: Город находится в области с небольшим представител...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Как раньше назывался город Киров?\"\n",
    "print(f\"Запрос: {query}\")\n",
    "\n",
    "query_embedding = get_embeddings([query], embedding_model, embedding_tokenizer, device, 1)[0]\n",
    "results = vector_db.search(query_embedding, top_k=3)\n",
    "\n",
    "print(f\"Найденные результаты:\")\n",
    "print()\n",
    "for idx, (chunk_idx, similarity) in enumerate(results, 1):\n",
    "    metadata = chunk_metadata[chunk_idx]\n",
    "    print(f\"Результат №{idx}. Сходство: {similarity:.3f}\")\n",
    "    print(f\"Заголовок: {metadata['article_name']}\")\n",
    "    print(f\"Текст: {metadata['chunk_text'][:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba99f278-6158-46ca-99b7-2b58a2af62ad",
   "metadata": {},
   "source": [
    "Теперь надо придумать промпт, который будет лучше всего помогать модели генерировать ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b4eb508-84f3-46b0-b32b-9aa0bbedee7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:36:46.599189Z",
     "iopub.status.busy": "2026-02-15T19:36:46.598973Z",
     "iopub.status.idle": "2026-02-15T19:36:46.614091Z",
     "shell.execute_reply": "2026-02-15T19:36:46.613249Z",
     "shell.execute_reply.started": "2026-02-15T19:36:46.599164Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_rag_answer(query: str, \n",
    "                        vector_db: LSHVectorDB,\n",
    "                        chunk_metadata: List[dict],\n",
    "                        embedding_model=None,\n",
    "                        embedding_tokenizer=None,\n",
    "                        device=None,\n",
    "                        generation_model=None,\n",
    "                        generation_tokenizer=None,\n",
    "                        top_k: int = 5, \n",
    "                        max_context_length: int = 5000,\n",
    "                        temperature: float = 0.7,\n",
    "                        top_p: float = 0.9,\n",
    "                        max_new_tokens: int = 512) -> str:    \n",
    "    query_embedding = get_embeddings([query], embedding_model, embedding_tokenizer, device, batch_size=1)[0]\n",
    "    search_results = vector_db.search(query_embedding, top_k=top_k)\n",
    "    \n",
    "    if not search_results:\n",
    "        return \"Извините, не удалось найти релевантную информацию в базе знаний\"\n",
    "    \n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    \n",
    "    total_length = 0\n",
    "    seen_articles = set()\n",
    "    \n",
    "    for idx, (chunk_idx, similarity) in enumerate(search_results, 1):\n",
    "        metadata = chunk_metadata[chunk_idx]\n",
    "        \n",
    "        article_idx = metadata['article_idx']\n",
    "        article_name = metadata['article_name']\n",
    "        chunk_text = metadata['chunk_text']\n",
    "        \n",
    "        if article_idx in seen_articles:\n",
    "            continue\n",
    "        \n",
    "        article_info = f\"Статья №{idx}: {article_name}. Содержание: {chunk_text}\"\n",
    "        source_info = f\"{idx}. {article_name} (сходство: {similarity:.3f})\"\n",
    "        \n",
    "        article_length = len(article_info)\n",
    "        if total_length + article_length > max_context_length and context_parts:\n",
    "            break\n",
    "        \n",
    "        context_parts.append(article_info)\n",
    "        sources.append(source_info)\n",
    "        seen_articles.add(article_idx)\n",
    "        total_length += article_length\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Ты информационный ассистент. Твоя задача — отвечать на вопросы, используя ТОЛЬКО информацию из предоставленных ниже фрагментов статей Wikipedia.\n",
    "\n",
    "    ВАЖНО:\n",
    "    - Используй ТОЛЬКО информацию из приведённых фрагментов\n",
    "    - Не добавляй знаний из памяти\n",
    "    - Не придумывай факты\n",
    "    - Если в тексте нет ответа — прямо скажи, что информации недостаточно\n",
    "    - Отвечай чётко, кратко и по существу\n",
    "    \n",
    "    Фрагменты статей:\n",
    "    {context}\n",
    "    \n",
    "    Вопрос пользователя: {query}\n",
    "    \n",
    "    Ответ:\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = generation_tokenizer.apply_chat_template(messages,\n",
    "                                                    tokenize=False,\n",
    "                                                    add_generation_prompt=True)\n",
    "    inputs = generation_tokenizer(text, return_tensors=\"pt\").to(generation_model.device)\n",
    "    \n",
    "    prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = generation_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=generation_tokenizer.eos_token_id,\n",
    "            eos_token_id=generation_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    new_tokens = out[0, prompt_len:]\n",
    "    return generation_tokenizer.decode(new_tokens, skip_special_tokens=True).strip(), sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08337868-d153-44a6-b31f-536316f21288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:36:46.616327Z",
     "iopub.status.busy": "2026-02-15T19:36:46.616102Z",
     "iopub.status.idle": "2026-02-15T19:36:48.238875Z",
     "shell.execute_reply": "2026-02-15T19:36:48.238211Z",
     "shell.execute_reply.started": "2026-02-15T19:36:46.616303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Среднегодовая температура воздуха в России\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 54.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: −5,5 °C\n",
      "Найденные статьи:\n",
      "1. Екатеринбург (сходство: 0.811)\n",
      "2. Россия (сходство: 0.801)\n",
      "3. Воронеж (сходство: 0.795)\n",
      "4. Москва (сходство: 0.791)\n",
      "5. Калининград (сходство: 0.791)\n"
     ]
    }
   ],
   "source": [
    "query = \"Среднегодовая температура воздуха в России\" \n",
    "print(f\"Запрос: {query}\")\n",
    "\n",
    "answer_with_rag, sources = generate_rag_answer(\n",
    "            query=query,\n",
    "            vector_db=vector_db,\n",
    "            chunk_metadata=chunk_metadata,\n",
    "            embedding_model=embedding_model,\n",
    "            embedding_tokenizer=embedding_tokenizer,\n",
    "            device=device,\n",
    "            generation_model=generation_model,\n",
    "            generation_tokenizer=generation_tokenizer,\n",
    "            top_k=5,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9)\n",
    "print(f\"Ответ: {answer_with_rag}\")\n",
    "\n",
    "print(\"Найденные статьи:\")\n",
    "for source in sources:\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b470d616-a889-4b24-a807-60e0ad21d456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:36:48.242111Z",
     "iopub.status.busy": "2026-02-15T19:36:48.241805Z",
     "iopub.status.idle": "2026-02-15T19:36:50.599202Z",
     "shell.execute_reply": "2026-02-15T19:36:50.598518Z",
     "shell.execute_reply.started": "2026-02-15T19:36:48.242087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Где располагался административный центр войска Донского?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 58.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: Штаб Донского казачьего войска располагался в городах Черкасске и затем в Новочеркасске.\n",
      "Найденные статьи:\n",
      "1. Донские казаки (сходство: 0.821)\n",
      "3. Гражданская война в России (сходство: 0.799)\n"
     ]
    }
   ],
   "source": [
    "query = \"Где располагался административный центр войска Донского?\" \n",
    "print(f\"Запрос: {query}\")\n",
    "\n",
    "answer_with_rag, sources = generate_rag_answer(\n",
    "            query=query,\n",
    "            vector_db=vector_db,\n",
    "            chunk_metadata=chunk_metadata,\n",
    "            embedding_model=embedding_model,\n",
    "            embedding_tokenizer=embedding_tokenizer,\n",
    "            device=device,\n",
    "            generation_model=generation_model,\n",
    "            generation_tokenizer=generation_tokenizer,\n",
    "            top_k=5,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9)\n",
    "print(f\"Ответ: {answer_with_rag}\")\n",
    "\n",
    "print(\"Найденные статьи:\")\n",
    "for source in sources:\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a89813f9-c941-40b1-a26c-f52575adc0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:36:50.600410Z",
     "iopub.status.busy": "2026-02-15T19:36:50.600137Z",
     "iopub.status.idle": "2026-02-15T19:36:55.468941Z",
     "shell.execute_reply": "2026-02-15T19:36:55.468298Z",
     "shell.execute_reply.started": "2026-02-15T19:36:50.600376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Как раньше назывался город Киров?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 61.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: Первоначально город Киров назывался Вятка. Затем в 1780 году было переимянено в Вятку, и в декабре 1934 года, после убийства С.М. Кирова, в городах и районах с его именем были проведены реформы и город был переименован в Киров.\n",
      "Найденные статьи:\n",
      "1. Киров (сходство: 0.783)\n",
      "2. Киров (значения) (сходство: 0.775)\n",
      "5. Климовск (сходство: 0.741)\n"
     ]
    }
   ],
   "source": [
    "query = \"Как раньше назывался город Киров?\" \n",
    "print(f\"Запрос: {query}\")\n",
    "\n",
    "answer_with_rag, sources = generate_rag_answer(\n",
    "            query=query,\n",
    "            vector_db=vector_db,\n",
    "            chunk_metadata=chunk_metadata,\n",
    "            embedding_model=embedding_model,\n",
    "            embedding_tokenizer=embedding_tokenizer,\n",
    "            device=device,\n",
    "            generation_model=generation_model,\n",
    "            generation_tokenizer=generation_tokenizer,\n",
    "            top_k=5,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9)\n",
    "print(f\"Ответ: {answer_with_rag}\")\n",
    "\n",
    "print(\"Найденные статьи:\")\n",
    "for source in sources:\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d981fe9-d110-44c5-bb69-14ba6165cf85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:36:55.470320Z",
     "iopub.status.busy": "2026-02-15T19:36:55.469786Z",
     "iopub.status.idle": "2026-02-15T19:36:57.754395Z",
     "shell.execute_reply": "2026-02-15T19:36:57.753648Z",
     "shell.execute_reply.started": "2026-02-15T19:36:55.470295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Какая сложность у Self-Attention?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 56.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: Информация о сложности Self-Attention не найдена в представленных статьях.\n",
      "Найденные статьи:\n",
      "1. Бэкон, Фрэнсис (сходство: 0.718)\n",
      "2. Ислам (сходство: 0.711)\n",
      "3. Гарднер, Эрл Стэнли (сходство: 0.711)\n",
      "4. Шеллинг, Фридрих Вильгельм Йозеф (сходство: 0.707)\n",
      "5. Психология (сходство: 0.706)\n"
     ]
    }
   ],
   "source": [
    "query = \"Какая сложность у Self-Attention?\"\n",
    "print(f\"Запрос: {query}\")\n",
    "\n",
    "answer_with_rag, sources = generate_rag_answer(\n",
    "            query=query,\n",
    "            vector_db=vector_db,\n",
    "            chunk_metadata=chunk_metadata,\n",
    "            embedding_model=embedding_model,\n",
    "            embedding_tokenizer=embedding_tokenizer,\n",
    "            device=device,\n",
    "            generation_model=generation_model,\n",
    "            generation_tokenizer=generation_tokenizer,\n",
    "            top_k=5,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9)\n",
    "print(f\"Ответ: {answer_with_rag}\")\n",
    "\n",
    "print(\"Найденные статьи:\")\n",
    "for source in sources:\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "349fb678-4a18-4293-ad49-3d28ec3755ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:36:57.755743Z",
     "iopub.status.busy": "2026-02-15T19:36:57.755433Z",
     "iopub.status.idle": "2026-02-15T19:37:00.343371Z",
     "shell.execute_reply": "2026-02-15T19:37:00.342754Z",
     "shell.execute_reply.started": "2026-02-15T19:36:57.755712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Дворец Советов это что?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 57.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: Информация о Дворце Советов недоступна в указанных фрагментах статей.\n",
      "Найденные статьи:\n",
      "1. Зимний дворец (сходство: 0.729)\n",
      "2. Российская империя (сходство: 0.725)\n",
      "3. Дворцовая площадь (сходство: 0.719)\n",
      "5. Москва (сходство: 0.715)\n"
     ]
    }
   ],
   "source": [
    "query = \"Дворец Советов это что?\"\n",
    "print(f\"Запрос: {query}\")\n",
    "\n",
    "answer_with_rag, sources = generate_rag_answer(\n",
    "            query=query,\n",
    "            vector_db=vector_db,\n",
    "            chunk_metadata=chunk_metadata,\n",
    "            embedding_model=embedding_model,\n",
    "            embedding_tokenizer=embedding_tokenizer,\n",
    "            device=device,\n",
    "            generation_model=generation_model,\n",
    "            generation_tokenizer=generation_tokenizer,\n",
    "            top_k=5,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9)\n",
    "print(f\"Ответ: {answer_with_rag}\")\n",
    "\n",
    "print(\"Найденные статьи:\")\n",
    "for source in sources:\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7e7f5e9-c62e-4208-a9ba-91f90e717edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:37:00.344462Z",
     "iopub.status.busy": "2026-02-15T19:37:00.344195Z",
     "iopub.status.idle": "2026-02-15T19:37:03.389040Z",
     "shell.execute_reply": "2026-02-15T19:37:03.388362Z",
     "shell.execute_reply.started": "2026-02-15T19:37:00.344437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Калорийность салата Цезарь\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 54.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: Информация о калорийности салата Цезарь недоступна в предоставленных фрагментах статей.\n",
      "Найденные статьи:\n",
      "1. Солнце (сходство: 0.669)\n",
      "2. Международная система единиц (сходство: 0.660)\n",
      "5. Аргинин (сходство: 0.650)\n"
     ]
    }
   ],
   "source": [
    "query = \"Калорийность салата Цезарь\"\n",
    "print(f\"Запрос: {query}\")\n",
    "\n",
    "answer_with_rag, sources = generate_rag_answer(\n",
    "            query=query,\n",
    "            vector_db=vector_db,\n",
    "            chunk_metadata=chunk_metadata,\n",
    "            embedding_model=embedding_model,\n",
    "            embedding_tokenizer=embedding_tokenizer,\n",
    "            device=device,\n",
    "            generation_model=generation_model,\n",
    "            generation_tokenizer=generation_tokenizer,\n",
    "            top_k=5,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9)\n",
    "print(f\"Ответ: {answer_with_rag}\")\n",
    "\n",
    "print(\"Найденные статьи:\")\n",
    "for source in sources:\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328211a7-c638-4ea9-a3b7-c27e593e3e77",
   "metadata": {},
   "source": [
    "На первые три вопроса модель ответила правильные, на следующие три затруднилась дать ответ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FPYS2xw7Q-Kx",
   "metadata": {
    "id": "FPYS2xw7Q-Kx"
   },
   "source": [
    "## Поиск в интернете\n",
    "\n",
    "Как вы могли заметить, языковая модель с использованием RAG работает значительно лучше, чем без него. Тем не менее, в ряде случаев она всё же затрудняется дать корректный ответ. Качество RAG-подхода определяется не только архитектурой системы, но и полнотой используемого корпуса знаний, а также качеством моделей эмбеддингов и генерации. Поскольку наша база данных была сокращена, в ней могут отсутствовать отдельные факты или целые статьи. В таких ситуациях модель не способна найти релевантный фрагмент и, соответственно, не может корректно ответить на запрос.\n",
    "\n",
    "Для повышения качества RAG-системы мы добавим механизм автоматического обращения к интернету. Процесс извлечения релевантной информации будет устроен следующим образом: сначала выполняется поиск по локальной базе знаний и если наиболее релевантный фрагмент имеет сходство ниже заданного порога, то система считает, что локальной информации недостаточно, и инициирует веб-поиск.\n",
    "\n",
    "При веб-поиске используется поисковая выдача (DuckDuckGo), уже отсортированная по релевантности запросу. Сначала извлекаются заголовки и сниппеты найденных страниц. Для первых нескольких ссылок дополнительно выполняется попытка парсинга HTML-страницы с извлечением основного текстового содержимого. Полученные тексты (сниппеты и, при возможности, фрагменты страниц) объединяются и используются как контекст для генерации ответа.\n",
    "\n",
    "По сути, система работает в двух режимах:\n",
    "\n",
    "* если локальная база обладает достаточной уверенностью, то ответ формируется исключительно на её основе\n",
    "* если уверенность низкая, используется информация из интернета\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb597eb5-109e-4fda-b56b-b5a1d0375f3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:37:03.390130Z",
     "iopub.status.busy": "2026-02-15T19:37:03.389851Z",
     "iopub.status.idle": "2026-02-15T19:37:03.657286Z",
     "shell.execute_reply": "2026-02-15T19:37:03.656732Z",
     "shell.execute_reply.started": "2026-02-15T19:37:03.390107Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_web_only(\n",
    "    query: str,\n",
    "    max_results: int = 5,\n",
    "    parse_top_n: int = 2,\n",
    "    max_page_chars: int = 2000,\n",
    "    timeout: int = 10\n",
    ") -> Tuple[str, List[str]]:\n",
    "    print(f\"Поиск в интернете: '{query}'...\")\n",
    "\n",
    "    with DDGS() as ddgs:\n",
    "        results = list(ddgs.text(query, max_results=max_results))\n",
    "\n",
    "    if not results:\n",
    "        return \"\", [\"Интернет: результатов не найдено\"]\n",
    "\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "\n",
    "    for i, r in enumerate(results, 1):\n",
    "        url = r.get(\"href\") or r.get(\"url\") or \"\"\n",
    "        title = (r.get(\"title\") or \"\").strip()\n",
    "        snippet = (r.get(\"body\") or \"\").strip()\n",
    "\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        page_text = \"\"\n",
    "        if i <= parse_top_n:\n",
    "            try:\n",
    "                headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "                resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "                if resp.ok and \"text/html\" in (resp.headers.get(\"Content-Type\", \"\") or \"\"):\n",
    "                    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "                    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "                        tag.decompose()\n",
    "\n",
    "                    paras = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "                    page_text = \" \".join([p for p in paras if len(p) >= 40])\n",
    "                    page_text = page_text[:max_page_chars] if page_text else \"\"\n",
    "            except Exception:\n",
    "                page_text = \"\"\n",
    "\n",
    "        text = \"\\n\".join([x for x in [title, snippet, page_text] if x]).strip()\n",
    "        if len(text) < 30:\n",
    "            continue\n",
    "\n",
    "        context_parts.append(\n",
    "            f\"Источник: Интернет\\n\"\n",
    "            f\"URL: {url}\\n\"\n",
    "            f\"Фрагмент: {text}\"\n",
    "        )\n",
    "        sources.append(f\"{i}. {title or url} (web)\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    return \"\\n\\n\".join(context_parts), sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39a655fd-552e-40ba-8002-6386e2fe00c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:37:03.658821Z",
     "iopub.status.busy": "2026-02-15T19:37:03.658185Z",
     "iopub.status.idle": "2026-02-15T19:37:03.670532Z",
     "shell.execute_reply": "2026-02-15T19:37:03.669943Z",
     "shell.execute_reply.started": "2026-02-15T19:37:03.658794Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_rag_answer_with_web(\n",
    "    query: str,\n",
    "    vector_db: LSHVectorDB,\n",
    "    chunk_metadata: List[dict],\n",
    "    embedding_model=None,\n",
    "    embedding_tokenizer=None,\n",
    "    device=None,\n",
    "    generation_model=None,\n",
    "    generation_tokenizer=None,\n",
    "    top_k: int = 5,\n",
    "    max_context_length: int = 5000,\n",
    "    web_threshold: float = 0.75,\n",
    "    web_max_results: int = 5,\n",
    "    web_parse_top_n: int = 2,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    max_new_tokens: int = 512\n",
    "):\n",
    "    query_embedding = get_embeddings([query], embedding_model, embedding_tokenizer, device, batch_size=1)[0]\n",
    "    search_results = vector_db.search(query_embedding, top_k=top_k)\n",
    "\n",
    "    best_similarity = float(search_results[0][1]) if search_results else 0.0\n",
    "\n",
    "    if best_similarity < web_threshold:\n",
    "        web_context, web_sources = search_web_only(\n",
    "            query=query,\n",
    "            max_results=web_max_results,\n",
    "            parse_top_n=web_parse_top_n,\n",
    "            max_page_chars=min(2000, max_context_length),\n",
    "        )\n",
    "\n",
    "        if not web_context:\n",
    "            return \"Не удалось извлечь информацию из интернета.\", web_sources\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Ты информационный ассистент. Отвечай на вопрос, используя ТОЛЬКО информацию из источников ниже.\n",
    "        \n",
    "        ВАЖНО:\n",
    "        - Используй ТОЛЬКО информацию из источников\n",
    "        - Не добавляй знаний из памяти\n",
    "        - Не придумывай факты\n",
    "        - Если в источниках нет ответа — прямо скажи, что информации недостаточно\n",
    "        - Отвечай чётко, кратко и по существу\n",
    "        \n",
    "        Источники:\n",
    "        {web_context}\n",
    "        \n",
    "        Вопрос пользователя: {query}\n",
    "        \n",
    "        Ответ:\n",
    "        \"\"\".strip()\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        text = generation_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = generation_tokenizer(text, return_tensors=\"pt\").to(generation_model.device)\n",
    "\n",
    "        prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "        with torch.no_grad():\n",
    "            out = generation_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=generation_tokenizer.eos_token_id,\n",
    "                eos_token_id=generation_tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        answer = generation_tokenizer.decode(out[0, prompt_len:], skip_special_tokens=True).strip()\n",
    "        return answer, web_sources\n",
    "\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    total_length = 0\n",
    "    seen_articles = set()\n",
    "\n",
    "    for idx, (chunk_idx, similarity) in enumerate(search_results, 1):\n",
    "        md = chunk_metadata[chunk_idx]\n",
    "        article_idx = md[\"article_idx\"]\n",
    "        if article_idx in seen_articles:\n",
    "            continue\n",
    "\n",
    "        article_name = md[\"article_name\"]\n",
    "        chunk_text = md[\"chunk_text\"]\n",
    "\n",
    "        article_info = f\"Статья №{idx}: {article_name}. Содержание: {chunk_text}\"\n",
    "        if total_length + len(article_info) > max_context_length and context_parts:\n",
    "            break\n",
    "\n",
    "        context_parts.append(article_info)\n",
    "        sources.append(f\"{idx}. {article_name} (сходство: {float(similarity):.3f})\")\n",
    "        seen_articles.add(article_idx)\n",
    "        total_length += len(article_info)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Ты информационный ассистент. Твоя задача — отвечать на вопросы, используя ТОЛЬКО информацию из предоставленных ниже фрагментов статей Wikipedia.\n",
    "    \n",
    "    ВАЖНО:\n",
    "    - Используй ТОЛЬКО информацию из приведённых фрагментов\n",
    "    - Не добавляй знаний из памяти\n",
    "    - Не придумывай факты\n",
    "    - Если в тексте нет ответа — прямо скажи, что информации недостаточно\n",
    "    - Отвечай чётко, кратко и по существу\n",
    "    \n",
    "    Фрагменты статей:\n",
    "    {context}\n",
    "    \n",
    "    Вопрос пользователя: {query}\n",
    "    \n",
    "    Ответ:\n",
    "    \"\"\".strip()\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = generation_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = generation_tokenizer(text, return_tensors=\"pt\").to(generation_model.device)\n",
    "\n",
    "    prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "    with torch.no_grad():\n",
    "        out = generation_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=generation_tokenizer.eos_token_id,\n",
    "            eos_token_id=generation_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    answer = generation_tokenizer.decode(out[0, prompt_len:], skip_special_tokens=True).strip()\n",
    "    return answer, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c42f9f98-ea3e-4fcf-8244-d58ea2db1359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:37:03.671765Z",
     "iopub.status.busy": "2026-02-15T19:37:03.671456Z",
     "iopub.status.idle": "2026-02-15T19:37:26.754833Z",
     "shell.execute_reply": "2026-02-15T19:37:26.754154Z",
     "shell.execute_reply.started": "2026-02-15T19:37:03.671742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Какая сложность у Self-Attention?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 57.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск в интернете: 'Какая сложность у Self-Attention?'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:primp.impersonate:Impersonate 'firefox_128' does not exist, using 'random'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ответ:\n",
      "Вычислительная сложность Self-Attention в зависимости от конкретной реализации может варьироваться. Наивная сложность по длине последовательности \\(n\\) составляет \\(O(n^2)\\), так как требуется построение полной матрицы внимания размером \\(n \\times n\\).\n",
      "\n",
      "Однако, в практике часто используется более эффективная реализация, где сложность уменьшается до \\(O(n \\cdot d + n \\cdot d^2)\\), где \\(d\\) - размерность скрытого представления. Это достигается путем кэширования результатов вычислений и использования параллелизма.\n",
      "\n",
      "Для двунаправленных RNN также требуется полное прогонение последовательности, что приводит к сложности \\(O(n^2)\\), что делает Self-Attention по сравнению с RNN более масштабируемым.\n",
      "\n",
      "Использованные источники:\n",
      "1. Attention и трансформеры в NLP: что спрашивают на... - DeepSchool (web)\n",
      "2. машинное обучение - Вычислительная сложность самоконтроля... (web)\n",
      "3. Трансформеры (web)\n",
      "4. Теормин по курсу “Генеративные модели машинного обучения”... (web)\n",
      "5. «ИИ без границ»: как научить Transformer обрабатывать... / Хабр (web)\n"
     ]
    }
   ],
   "source": [
    "query = \"Какая сложность у Self-Attention?\"\n",
    "print(f\"Запрос: {query}\")\n",
    "\n",
    "answer, sources = generate_rag_answer_with_web(\n",
    "    query=query,\n",
    "    vector_db=vector_db,\n",
    "    chunk_metadata=chunk_metadata,\n",
    "    embedding_model=embedding_model,\n",
    "    embedding_tokenizer=embedding_tokenizer,\n",
    "    device=device,\n",
    "    generation_model=generation_model,\n",
    "    generation_tokenizer=generation_tokenizer,\n",
    "    top_k=5,\n",
    "    web_threshold=0.75,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(\"\\nОтвет:\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\nИспользованные источники:\")\n",
    "for s in sources:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6df31f91-e7d6-47f3-8f65-f8c8a3451c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:37:26.755905Z",
     "iopub.status.busy": "2026-02-15T19:37:26.755691Z",
     "iopub.status.idle": "2026-02-15T19:37:40.150172Z",
     "shell.execute_reply": "2026-02-15T19:37:40.149368Z",
     "shell.execute_reply.started": "2026-02-15T19:37:26.755871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Дворец Советов это что?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 57.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск в интернете: 'Дворец Советов это что?'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ответ:\n",
      "Дворец Советов был неосуществлённым проектом строительства высотного административного здания в Москве для проведения сессий Верховного Совета СССР. Проект начался в 1937 году, но был остановлен вскоре после начала Второй мировой войны. Здание планировалось стать самой высокой в мире, однако работы были прекращены перед тем, как строительство достигло половины завершения.\n",
      "\n",
      "Использованные источники:\n",
      "1. Кто сказал, что это Бурдж Халифа?! Это Дворец Советов) (web)\n",
      "2. Грандиозный проект СССР: Дворец Советов | TikTok (web)\n",
      "3. Palace Of Soviets Дворец советов Minecraft Map (web)\n",
      "4. Palace of the Soviets (Moscow) Russia. began to be built in 1937, it ... (web)\n",
      "5. Steam Workshop::Palace of Soviet (web)\n"
     ]
    }
   ],
   "source": [
    "query = \"Дворец Советов это что?\"\n",
    "print(f\"Запрос: {query}\")\n",
    "\n",
    "answer, sources = generate_rag_answer_with_web(\n",
    "    query=query,\n",
    "    vector_db=vector_db,\n",
    "    chunk_metadata=chunk_metadata,\n",
    "    embedding_model=embedding_model,\n",
    "    embedding_tokenizer=embedding_tokenizer,\n",
    "    device=device,\n",
    "    generation_model=generation_model,\n",
    "    generation_tokenizer=generation_tokenizer,\n",
    "    top_k=5,\n",
    "    web_threshold=0.75,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(\"\\nОтвет:\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\nИспользованные источники:\")\n",
    "for s in sources:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e767563e-0748-480c-813b-d5dfafa152e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:37:40.151939Z",
     "iopub.status.busy": "2026-02-15T19:37:40.151194Z",
     "iopub.status.idle": "2026-02-15T19:37:51.893387Z",
     "shell.execute_reply": "2026-02-15T19:37:51.892714Z",
     "shell.execute_reply.started": "2026-02-15T19:37:40.151904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Калорийность салата Цезарь\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 60.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск в интернете: 'Калорийность салата Цезарь'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ответ:\n",
      "Калорийность салата Цезарь может варьироваться в зависимости от ингредиентов. Например, по рецепту, описанному в одном из источников, салат содержит 45.38 ккал на 100 грамм. Однако без полного списка ингредиентов и их количества невозможно дать точную калорийность.\n",
      "\n",
      "Использованные источники:\n",
      "1. recepti_pp (@recepti_pp07) • Instagram photos and videos (web)\n",
      "2. Delicious Caesar Salad and Diet Coke Combo - TikTok (web)\n",
      "3. Kale & White Bean Caesar Salad Recipe - TikTok (web)\n",
      "4. Bang Bang Chicken Fries Recipe for Weight Loss! - TikTok (web)\n",
      "5. Ushnoe (stew beef with potatoes and vegetables) (web)\n"
     ]
    }
   ],
   "source": [
    "query = \"Калорийность салата Цезарь\"\n",
    "print(f\"Запрос: {query}\")\n",
    "\n",
    "answer, sources = generate_rag_answer_with_web(\n",
    "    query=query,\n",
    "    vector_db=vector_db,\n",
    "    chunk_metadata=chunk_metadata,\n",
    "    embedding_model=embedding_model,\n",
    "    embedding_tokenizer=embedding_tokenizer,\n",
    "    device=device,\n",
    "    generation_model=generation_model,\n",
    "    generation_tokenizer=generation_tokenizer,\n",
    "    top_k=5,\n",
    "    web_threshold=0.75,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(\"\\nОтвет:\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\nИспользованные источники:\")\n",
    "for s in sources:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bdfbc4-44b6-42b8-bdf8-a05da6f191f2",
   "metadata": {},
   "source": [
    "Теперь ответы модели стали правильными, поскольку модель получает информацию, которой не было в исходном датасете, из интернета.\n",
    "\n",
    "Возникает закономерный вопрос: если система может обращаться к интернету, зачем вообще использовать локальную базу знаний? Дело в том, что локальная база обеспечивает предсказуемость, стабильность и контроль качества источников. Она не зависит от внешних сервисов, скорости сети или изменений в содержимом сайтов, а также позволяет ограничить модель рамками заранее определённого и проверенного корпуса текстов.\n",
    "\n",
    "Интернет в данной архитектуре выступает как механизм расширения, а не замены базы. Он подключается только при недостаточной уверенности локального поиска и помогает компенсировать неполноту корпуса."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41114f3-c204-49b0-aa2c-bc51006e4638",
   "metadata": {},
   "source": [
    "## Уточняющие вопросы\n",
    "\n",
    "Одним из ограничений текущей системы является отсутствие полноценной поддержки уточняющих вопросов. Если пользователь задаёт вопрос, ссылающийся на предыдущий ответ (например, \"А сколько времени это занимает?\"), система не понимает, к чему относится местоимение, и вынуждает пользователя переписывать запрос полностью. Это снижает удобство использования.\n",
    "\n",
    "Можно было бы сохранять всю историю диалога и каждый раз передавать её в промпт генерации. Однако такой подход значительно увеличивает размер входного текста, что приводит к росту затрат по времени и памяти, а также повышает риск “зашумления” контекста. Вместо этого в нашей реализации используется более эффективная схема: перед этапом поиска релевантных фрагментов система сохраняет несколько последних пар вопрос–ответ и просит модель переформулировать текущий запрос так, чтобы он стал самодостаточным.\n",
    "\n",
    "Например:\n",
    "\"Какая у него калорийность?\" -> \"Какая калорийность у салата Цезарь?\"\n",
    "\n",
    "После этого поиск по векторной базе (и, при необходимости, в интернете) выполняется уже по переформулированному запросу. Наша система учитывает историю диалога, не увеличивая при этом размер финального промпта для генерации ответа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4471c5ba-0b12-4d47-8104-0d11e3f848e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T20:02:13.686223Z",
     "iopub.status.busy": "2026-02-15T20:02:13.685563Z",
     "iopub.status.idle": "2026-02-15T20:02:13.698707Z",
     "shell.execute_reply": "2026-02-15T20:02:13.698081Z",
     "shell.execute_reply.started": "2026-02-15T20:02:13.686193Z"
    }
   },
   "outputs": [],
   "source": [
    "class RAGChat:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_db,\n",
    "        chunk_metadata: List[dict],\n",
    "        embedding_model,\n",
    "        embedding_tokenizer,\n",
    "        device,\n",
    "        generation_model,\n",
    "        generation_tokenizer,\n",
    "        history_max_turns: int = 3,\n",
    "        answer_snippet_chars: int = 200,\n",
    "    ):\n",
    "        self.vector_db = vector_db\n",
    "        self.chunk_metadata = chunk_metadata\n",
    "\n",
    "        self.embedding_model = embedding_model\n",
    "        self.embedding_tokenizer = embedding_tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        self.generation_model = generation_model\n",
    "        self.generation_tokenizer = generation_tokenizer\n",
    "\n",
    "        self.history_max_turns = history_max_turns\n",
    "        self.answer_snippet_chars = answer_snippet_chars\n",
    "\n",
    "        self.history: List[Tuple[str, str]] = []\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.history.clear()\n",
    "\n",
    "    def add_exchange(self, question: str, answer: str) -> None:\n",
    "        self.history.append((question, answer))\n",
    "\n",
    "    def get_recent(self, n: Optional[int] = None) -> List[Tuple[str, str]]:\n",
    "        n = self.history_max_turns if n is None else n\n",
    "        return self.history[-n:] if len(self.history) > n else list(self.history)\n",
    "\n",
    "    def _llm_generate(self, prompt: str, max_new_tokens: int = 96) -> str:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        text = self.generation_tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.generation_tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.generation_model.device) for k, v in inputs.items()}\n",
    "\n",
    "        prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            out = self.generation_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.generation_tokenizer.eos_token_id,\n",
    "                eos_token_id=self.generation_tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        new_tokens = out[0, prompt_len:]\n",
    "        return self.generation_tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    def reformulate_query(self, query: str) -> str:\n",
    "        if not self.history:\n",
    "            return query\n",
    "\n",
    "        recent = self.get_recent()\n",
    "\n",
    "        hist_lines = []\n",
    "        for i, (q, a) in enumerate(recent, 1):\n",
    "            a_short = \" \".join(a.split())\n",
    "            if len(a_short) > self.answer_snippet_chars:\n",
    "                a_short = a_short[: self.answer_snippet_chars].rstrip() + \"…\"\n",
    "            hist_lines.append(f\"Q{i}: {q}\\nA{i}: {a_short}\")\n",
    "\n",
    "        history_text = \"\\n\\n\".join(hist_lines)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Ты помощник, который переформулирует вопрос пользователя так, чтобы он был самодостаточным.\n",
    "        \n",
    "        История (последние реплики):\n",
    "        {history_text}\n",
    "        \n",
    "        Текущий вопрос:\n",
    "        {query}\n",
    "        \n",
    "        Правила:\n",
    "        - Если есть ссылки на контекст (\"он\", \"она\", \"это\", \"у него\", \"у неё\", \"там\", \"такой\", \"тут\", \"здесь\") — уточни их по истории.\n",
    "        - Верни ТОЛЬКО переформулированный вопрос одной строкой, без кавычек и пояснений.\n",
    "        \n",
    "        Переформулированный вопрос:\n",
    "        \"\"\".strip()\n",
    "\n",
    "        rewritten = self._llm_generate(prompt, max_new_tokens=96)\n",
    "        rewritten = (rewritten.splitlines()[0] if rewritten else \"\").strip().strip('\"').strip(\"'\")\n",
    "\n",
    "        if not rewritten or len(rewritten) < 3:\n",
    "            return query\n",
    "        bad = {\"assistant\", \"none\", \"null\"}\n",
    "        if rewritten.lower() in bad:\n",
    "            return query\n",
    "\n",
    "        return rewritten\n",
    "\n",
    "    def ask(\n",
    "        self,\n",
    "        query: str,\n",
    "        use_web: bool = True,\n",
    "        reformulate: bool = True,\n",
    "        **rag_kwargs: Any,\n",
    "    ) -> Tuple[str, List[str], str]:\n",
    "        used_query = self.reformulate_query(query) if reformulate else query\n",
    "\n",
    "        answer, sources = generate_rag_answer_with_web(\n",
    "            query=used_query,\n",
    "            vector_db=self.vector_db,\n",
    "            chunk_metadata=self.chunk_metadata,\n",
    "            embedding_model=self.embedding_model,\n",
    "            embedding_tokenizer=self.embedding_tokenizer,\n",
    "            device=self.device,\n",
    "            generation_model=self.generation_model,\n",
    "            generation_tokenizer=self.generation_tokenizer,\n",
    "            **rag_kwargs,\n",
    "        )\n",
    "\n",
    "        self.add_exchange(query, answer)\n",
    "\n",
    "        return answer, sources, used_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "831efa29-03fa-4d7f-b845-d285a0e074b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:51:44.182302Z",
     "iopub.status.busy": "2026-02-15T19:51:44.182035Z",
     "iopub.status.idle": "2026-02-15T19:51:44.186288Z",
     "shell.execute_reply": "2026-02-15T19:51:44.185559Z",
     "shell.execute_reply.started": "2026-02-15T19:51:44.182280Z"
    }
   },
   "outputs": [],
   "source": [
    "chat = RAGChat(\n",
    "    vector_db=vector_db,\n",
    "    chunk_metadata=chunk_metadata,\n",
    "    embedding_model=embedding_model,\n",
    "    embedding_tokenizer=embedding_tokenizer,\n",
    "    device=device,\n",
    "    generation_model=generation_model,\n",
    "    generation_tokenizer=generation_tokenizer,\n",
    "    history_max_turns=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e3c0414-94a8-4f48-a3f2-6299ca38fcf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:56:59.270013Z",
     "iopub.status.busy": "2026-02-15T19:56:59.269004Z",
     "iopub.status.idle": "2026-02-15T19:57:17.129549Z",
     "shell.execute_reply": "2026-02-15T19:57:17.128821Z",
     "shell.execute_reply.started": "2026-02-15T19:56:59.269982Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 55.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первоначально город Киров назывался Вятка, а затем Хлынов, перед переименованием в 1780 году в Вятку, и в 1934 году в Киров.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 69.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Город Киров был переименован в честь уроженца города Уржум Кировской губернии С. М. Кирова после его убийства.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 71.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск в интернете: 'Каково настоящее население города Киров?'...\n",
      "Население города Киров составляет 475 871 человек (2025 год).\n"
     ]
    }
   ],
   "source": [
    "chat.clear()\n",
    "\n",
    "q1 = \"Как раньше назывался город Киров?\"\n",
    "a1, s1, used1 = chat.ask(query=q1)\n",
    "print(a1)\n",
    "\n",
    "q2 = \"А в честь кого он был переименован?\"\n",
    "a2, s2, used2 = chat.ask(query=q2)\n",
    "print(a2)\n",
    "\n",
    "q3 = \"А какое у него население?\"\n",
    "a3, s3, used3 = chat.ask(query=q3)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5dd0d8e5-f53d-41fa-bed9-3b3f82a99d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T20:40:14.581209Z",
     "iopub.status.busy": "2026-02-15T20:40:14.580342Z",
     "iopub.status.idle": "2026-02-15T20:40:55.716562Z",
     "shell.execute_reply": "2026-02-15T20:40:55.715875Z",
     "shell.execute_reply.started": "2026-02-15T20:40:14.581177Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 55.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск в интернете: 'Что такое Дворец Советов?'...\n",
      "Дворец Советов — неосуществлённый проект строительства высотного административного здания в Москве для проведения сессий Верховного Совета СССР и массовых демонстраций. Проект предполагал высоту 415 метров, что делало его самым высоким зданием в мире. Однако строительство было приостановлено в 1941 году из-за Великой Отечественной войны и никогда полностью не осуществилось. Здание было возвестили на месте будущего храма Христа Спасителя. Проект проводился в 1931-1939 годах, но из-за войны работы были приостановлены, а стальные конструкции использованы для оборонительных работ.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 69.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск в интернете: 'На каком месте хотели построить Дворец Советов?'...\n",
      "На этом месте хотели построить Дворец Советов, но потом сделали самый большой в Европе открытый бассейн \"Москва\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 71.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск в интернете: 'Что должно было быть на вершине Дворца Советов?'...\n",
      "Стометровая статуя Владимира Ленина.\n"
     ]
    }
   ],
   "source": [
    "chat.clear()\n",
    "\n",
    "q1 = \"Что такое Дворец Советов?\"\n",
    "a1, s1, used1 = chat.ask(query=q1)\n",
    "print(a1)\n",
    "\n",
    "q2 = \"На каком месте его хотели построить?\"\n",
    "a2, s2, used2 = chat.ask(query=q2)\n",
    "print(a2)\n",
    "\n",
    "q3 = \"Что должно было быть на его вершине?\"\n",
    "a3, s3, used3 = chat.ask(query=q3)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d146b929-cf5e-4bcd-8937-ec3af0047c2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T20:43:31.600687Z",
     "iopub.status.busy": "2026-02-15T20:43:31.599895Z",
     "iopub.status.idle": "2026-02-15T20:44:14.109838Z",
     "shell.execute_reply": "2026-02-15T20:44:14.109206Z",
     "shell.execute_reply.started": "2026-02-15T20:43:31.600638Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 54.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск в интернете: 'В какой республике зарегестрирована ТатНефть?'...\n",
      "ТатНефть зарегистрирована в Республике Татарстан.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 66.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск в интернете: 'Какое население в Республике Татарстан?'...\n",
      "По данным Росстата на 1 января 2025 года численность населения Республики Татарстан составляла 4 016 571 человек.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Векторизация: 100%|██████████| 1/1 [00:00<00:00, 71.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск в интернете: 'Какая столица Республики Татарстан?'...\n",
      "Столица Республики Татарстан - город Казань.\n"
     ]
    }
   ],
   "source": [
    "chat.clear()\n",
    "\n",
    "q1 = \"В какой республике зарегестрирована ТатНефть?\"\n",
    "a1, s1, used1 = chat.ask(query=q1)\n",
    "print(a1)\n",
    "\n",
    "q2 = \"Какое в этой республике население?\"\n",
    "a2, s2, used2 = chat.ask(query=q2)\n",
    "print(a2)\n",
    "\n",
    "q3 = \"А какая у нее столица?\"\n",
    "a3, s3, used3 = chat.ask(query=q3)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccc2ed-5b0c-4004-ab0d-34a4f93e3ff4",
   "metadata": {},
   "source": [
    "Как мы видим модель четко улавливает контекст и правильно отвечает на вопросы. Иногда она может сомневаться в ответе, особенно когда опирается на локальную базу данных, однако так даже лучше, поскольку модель не галюцинирует."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78381ae3-cb6e-4b88-8b36-a2a5728532cf",
   "metadata": {},
   "source": [
    "## Идеи для улучшения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3711b4f-41be-4131-b251-acf812ac26a0",
   "metadata": {},
   "source": [
    "Текущая реализация уже вполне работоспособна, однако всегда надо стремиться к развитию. Чтобы повысить качество модели можно попробовать следующее:\n",
    "* перейти с LSH на HNSW, сейчас тексты на разные темы все равно имеют высокую схожесть. HNSW будет здесь даже в тему, если понять какие страницы связаны с какими, можно построить логичный граф.\n",
    "* Увеличить датасет, взять все строки. Также можно добавить в матаданные информацию о домене (география, математика и т.д.)\n",
    "* Взять модель для генерации с большим количеством параметров\n",
    "* Сделать CoT чтобы отвечать на сложные запросы"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9497122,
     "sourceId": 14848403,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
